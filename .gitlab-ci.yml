#Available stages to run in 'automatic' processing:
# For convenience, implicit stages have been made explicit here...
stages:
  - Initialization
  - Regular Installation
  - Basic Testing
  - Regular Testing
  - Benchmarks
  - Applications
  - Breaking tests
  - Configurations
  - Finalization
  - Release

# Add clone variables for concurent PIPELINES
variables:
  GL_PIPELINE_PATH: $CI_BUILDS_DIR/GL_$CI_PIPELINE_ID
  GL_BUILD_DIR: $GL_PIPELINE_PATH/build
  GL_INSTALL_DIR: $GL_PIPELINE_PATH/install
  GIT_DEPTH: 1 # No need to clone the whole history
  GIT_CLONE_PATH: $GL_PIPELINE_PATH/$CI_RUNNER_ID/$CI_CONCURRENT_ID
  PCVS_COMMAND: pcvs -v run --override --profile mpc --print errors

.pcvs_prepare: &pcvs_prepare
  - export CI_PCVS_DIR="$GL_PIPELINE_PATH/step_$(echo "$CI_JOB_NAME" | sed -e "s/ /_/g")/"
  - test -d "${CI_PCVS_DIR}" && rm -rf "${CI_PCVS_DIR}"
  - mkdir -p "${CI_PCVS_DIR}"
  - cd "${CI_PCVS_DIR}"
  - echo "Step run from ${CI_PCVS_DIR}"
  - source ${GL_INSTALL_DIR}/mpcvars.sh
  - echo "MPC loaded from ${GL_INSTALL_DIR}/mpcvars.sh"


# Template nodes to be inserted to any CI step.
# This ensure the pipeline will be triggered only for these scenarios.
# Theses lines should never be modified.
.run_by_main: &run_by_main
  only:
  - devel
  - merge_requests
  - web
  except:
    variables:
      # Skip MR tagged as WIP
      - $CI_MERGE_REQUEST_TITLE =~ /^WIP:/

.uses_slurm: &uses_slurm
  tags:
    - slurm


############################
##### EXTRA ACTIONS ########
############################
#these actions should herit from implicit stages ".pre" and ".post"

# pre-actions to cleanup the machine before the run
# this job will be run in ANY pipeline -> ensure to enable all proper tags
# CAUTION : Otherwise tag-specific runners won't allow to run the whole pipeline because
# both .pre and .post cannot be scheduled (thus, why not using user-defined pre and post
# to avoid them to be run systematically ?)
Env Sanitize:
  <<: *run_by_main
  stage: Initialization
  script:
    - test -d $GL_PIPELINE_PATH && rm -rf $GL_PIPELINE_PATH
    - mkdir -p $GL_PIPELINE_PATH

# post-actions to cleanup the machine after the run
# The last line may not be necessary as a pipeline start run something like a "git clean" before running
# This implies a probable issue when multiple piplines are run concurrently on the same project :(
# Please read the CAUTION above !
Artifact Deletion:
  <<: *run_by_main
  stage: Finalization
  allow_failure: true
  when: on_success
  script:
    - rm -rf $GL_PIPELINE_PATH
    - $HOME/clean_old_pipelines.sh

Resource Relinquishing:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Finalization
  allow_failure: true
  when: always
  script:
  #
  - scancel --name GL-build_$CI_PIPELINE_ID
  # Ugly, our way to kill allocations from tests
  - scancel -u $USER

############################
####### CONF STAGE  ########
############################
.conf_run_cmds: &configuration_commands
  - rm -rf ${GL_LOCAL_BUILD_DIR} ${GL_LOCAL_INSTALL_DIR}
  - mkdir ${GL_LOCAL_BUILD_DIR}; cd ${GL_LOCAL_BUILD_DIR}
  - ${CI_PROJECT_DIR}/installmpc -vv --force-spack-arch=x86_64_v2 --prefix=${GL_LOCAL_INSTALL_DIR} ${GL_INSTALL_OPTS}


MPC Default Configuration:
  <<: *run_by_main
  stage: Regular Installation
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}
    GL_INSTALL_OPTS: --mpc-option="--enable-debug" --with-pmi1
  script:
    - *configuration_commands


MPC Process Mode (no Spack):
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_nopriv_no_spack
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_nopriv
    GL_INSTALL_OPTS: --disable-spack --disable-mpc-autopriv
  script:
    - *configuration_commands



MPC Process Mode:
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_nopriv
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_nopriv
    GL_INSTALL_OPTS: --disable-mpc-autopriv
  script:
    - *configuration_commands

MPC Disable Lowcomm:
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_nolowcomm
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_nolowcomm
    GL_INSTALL_OPTS: --disable-mpc-autopriv --mpc-option="--disable-lowcomm"
  script:
    - *configuration_commands

MPC Disable MPI:
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_nompi
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_nompi
    GL_INSTALL_OPTS: --disable-mpc-autopriv --mpc-option="--disable-mpi"
  script:
    - *configuration_commands

MPC Disable Threads:
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_nothread
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_nothread
    GL_INSTALL_OPTS: --disable-mpc-autopriv --mpc-option="--disable-threads"
  script:
    - *configuration_commands

MPC Debug Messages:
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_debug
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_debug
    GL_INSTALL_OPTS: --disable-mpc-autopriv --enable-debug --mpc-option="--enable-debug-messages"
  script:
    - *configuration_commands

MPC Slurm:
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_slurm
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_slurm
    GL_INSTALL_OPTS: --disable-mpc-autopriv --with-slurm
  script:
    - *configuration_commands

MPC PMIx:
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_pmix
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_pmix
    GL_INSTALL_OPTS: --disable-mpc-autopriv --with-pmix
  script:
    - *configuration_commands

MPC Compilation Workshare:
  <<: *run_by_main
  stage: Configurations
  variables:
    GL_LOCAL_BUILD_DIR: ${GL_BUILD_DIR}_workshare
    GL_LOCAL_INSTALL_DIR: ${GL_INSTALL_DIR}_workshare
    GL_INSTALL_OPTS: --mpc-option="--enable-debug" --enable-workshare
  allow_failure: true
  script:
    - *configuration_commands

############################
####### TEST STAGE ########
############################

# a BASIC one, to stop the pipeline early in case of major issue (not able to compile/run a simple test)
Simple run:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Basic Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/trivial/


Privatization:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Basic Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/privatization/

#
## a regular test, to assess some basic MPC feature
#

MPI Simple C:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Regular Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/simple/c/

MPI Simple Fortran:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Regular Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/simple/fortran/

#MPI NBC:
#  <<: *run_by_main
#  <<: *uses_slurm
#  stage: Regular Testing
#  script:
#    - *pcvs_prepare
#    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/NBC/

Lowcomm:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Regular Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/lowcomm/

#
# Benchmarks
#
MPI IMB-MPI 2017:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Benchmarks
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/IMB_2017/check-mpi/

MPI NBC:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Benchmarks
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/NBC/

MPI IMB-NBC 2017:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Benchmarks
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/IMB_2017/check-nbc/

#
# Applications
#


Lulesh:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Applications
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/corals/lulesh-2.0.3/

miniFe:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Applications
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/corals/miniFe/

NAS-MZ MPI:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Applications
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/hybrid/NAS/MZ-MPI/

NAS-MZ OMP:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Applications
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/hybrid/NAS/MZ-OMP/


###################################
####### MANUALLY TRIGGERED ########
###################################
#This section gathers jobs not scheduled to be part of the standard pipeline.
#They could be run manually or periodically.

#When triggered through a variable, MPC is built in containers with fixed software stack (base compiler, libc, etc...)
# to ensure MPC to compile in a large variety of environments

Debian Stretch:
  stage: Regular Testing
  script: "docker run --rm -it paratoolsfrance/mpc-env:debian-stretch $HOME/docker/run_installmpc.sh"
  tags:
    - "docker"
  only:
    variables:
      - $MPC_CI_MODE == "multibuild"

Centos 7:
  stage: Regular Testing
  script: "docker run --rm -it paratoolsfrance/mpc-env:centos-7 $HOME/docker/run_installmpc.sh"
  tags:
    - "docker"
  only:
    variables:
      - $MPC_CI_MODE == "multibuild"

release:
  stage: Release
  when: manual
  script: utils/prepare_release.sh rpm --target=release/ --distribution=fedora:latest
  artifacts:
    paths:
      - release/x86_64
