#Available stages to run in 'automatic' processing:
# For convenience, implicit stages have been made explicit here...
stages:
  - Initialization
  - Configurations
  - Basic Testing
  - Regular Testing
  - Benchmarks
  - Applications
  - Breaking tests
  - Finalization

# Add clone variables for concurent PIPELINES
variables:
  GIT_DEPTH: 10 # No need to clone the whole history
  GIT_CLONE_PATH: $CI_BUILDS_DIR/$CI_RUNNER_ID/$CI_CONCURRENT_ID/$CI_PROJECT_PATH
  # PCVS CONF
  PCVS_COMMAND: pcvs -v run -f -p mpc


.pcvs_prepare: &pcvs_prepare
  - export CI_PCVS_DIR="$HOME/GL_${CI_PIPELINE_ID}/pcvs_run_$(echo $CI_JOB_NAME | sed "s/ /_/g")/"
  - test -d "${CI_PCVS_DIR}" || mkdir "${CI_PCVS_DIR}"
  - cd "${CI_PCVS_DIR}"
  - echo "PCVS RUN Directory is ${CI_PCVS_DIR}"
  - source $HOME/GL_$CI_PIPELINE_ID/install/mpcvars.sh
  - echo "SOURCING MPC from $HOME/GL_$CI_PIPELINE_ID/install/mpcvars.sh"

.run_by_main: &run_by_main
  only:
  - devel
  - pt_devel
  - merge_requests
  - web
  except:
    variables:
      - $CI_MERGE_REQUEST_TITLE =~ /^WIP:/

.breaking_tests: &breaking_tests
  allow_failure: true

.uses_slurm: &uses_slurm
  tags:
    - slurm




############################
##### EXTRA ACTIONS ########
############################
#these actions should herit from implicit stages ".pre" and ".post"

# pre-actions to cleanup the machine before the run
# this job will be run in ANY pipeline -> ensure to enable all proper tags
# CAUTION : Otherwise tag-specific runners won't allow to run the whole pipeline because
# both .pre and .post cannot be scheduled (thus, why not using user-defined pre and post
# to avoid them to be run systematically ?)
Env Sanitize:
  <<: *run_by_main
  stage: Initialization
  script:
    - mkdir -p $HOME/GL_$CI_PIPELINE_ID/{build,build_workshare,install,install_workshare,test}
    - echo "Environment Intialized !"

# post-actions to cleanup the machine after the run
# The last line may not be necessary as a pipeline start run something like a "git clean" before running
# This implies a probable issue when multiple piplines are run concurrently on the same project :(
# Please read the CAUTION above !
Artifact Deletion:
  <<: *run_by_main
  stage: Finalization
  allow_failure: true
  when: on_success
  script:
    - $HOME/clean_old_pipelines.sh
    - rm -rf $HOME/$CI_PIPELINE_ID/

Resource Relinquishing:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Finalization
  allow_failure: true
  when: always
  script:
  #
  - scancel --name GL-build_$CI_PIPELINE_ID
  # Ugly, our way to kill allocations from tests
  - scancel -u $USER

############################
####### CONF STAGE  ########
############################

MPC Compilation Workshare:
  <<: *run_by_main
  stage: Configurations
  script:
  - cd $HOME/GL_$CI_PIPELINE_ID/build_workshare
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_workshare --mpc-option='--with-pmi1 --enable-debug' --enable-workshare

MPC Default Configuration:
  <<: *run_by_main
  stage: Configurations
  script:
  - cd $HOME/GL_$CI_PIPELINE_ID/build
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install --mpc-option='--with-pmi1 --enable-debug'

MPC Process Mode:
  <<: *run_by_main
  stage: Configurations
  script:
  - mkdir $HOME/GL_$CI_PIPELINE_ID/build_nopriv/
  - cd $HOME/GL_$CI_PIPELINE_ID/build_nopriv/
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_process_mode --disable-mpc-autopriv

MPC Disable Lowcomm:
  <<: *run_by_main
  stage: Configurations
  script:
  - mkdir $HOME/GL_$CI_PIPELINE_ID/build_no_lowcomm/
  - cd $HOME/GL_$CI_PIPELINE_ID/build_no_lowcomm/
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_nolowcomm --disable-mpc-autopriv --mpc-option="--disable-lowcomm"


MPC Disable MPI:
  <<: *run_by_main
  stage: Configurations
  script:
  - mkdir $HOME/GL_$CI_PIPELINE_ID/build_nompi/
  - cd $HOME/GL_$CI_PIPELINE_ID/build_nompi/
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_nompi --disable-mpc-autopriv --mpc-option="--disable-mpi"


MPC Disable Threads:
  <<: *run_by_main
  stage: Configurations
  script:
  - mkdir $HOME/GL_$CI_PIPELINE_ID/build_nothread/
  - cd $HOME/GL_$CI_PIPELINE_ID/build_nothread/
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_nothread --disable-mpc-autopriv --mpc-option="--disable-threads"

MPC Debug Messages:
  <<: *run_by_main
  stage: Configurations
  script:
  - mkdir $HOME/GL_$CI_PIPELINE_ID/build_debug/
  - cd $HOME/GL_$CI_PIPELINE_ID/build_debug/
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_debug --disable-mpc-autopriv --mpc-option="--enable-debug-messages --enable-debug"

MPC Slurm:
  <<: *run_by_main
  stage: Configurations
  script:
  - mkdir $HOME/GL_$CI_PIPELINE_ID/build_slurm/
  - cd $HOME/GL_$CI_PIPELINE_ID/build_slurm/
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_slurm --disable-mpc-autopriv --mpc-option="--with-slurm"

MPC PMIx:
  <<: *run_by_main
  stage: Configurations
  script:
  - mkdir $HOME/GL_$CI_PIPELINE_ID/build_pmix/
  - cd $HOME/GL_$CI_PIPELINE_ID/build_pmix/
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_pmix --disable-mpc-autopriv --mpc-option="--with-pmix"

<<<<<<< HEAD
MPC Compilation Workshare:
  <<: *run_by_main
  stage: Configurations
  allow_failure: true
  script:
  - cd $HOME/GL_$CI_PIPELINE_ID/build_workshare
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install_workshare --mpc-option='--with-pmi1 --enable-debug' --enable-workshare

MPC Default Configuration:
  <<: *run_by_main
  stage: Configurations
  script:
  - cd $HOME/GL_$CI_PIPELINE_ID/build
  - ${CI_PROJECT_DIR}/installmpc -vv --disable-spack --prefix=$HOME/GL_$CI_PIPELINE_ID/install --with-slurm --mpc-option='--enable-debug'

=======
>>>>>>> 2d7191891 (CI: move large compilations first)
############################
####### TEST STAGE ########
############################

# a BASIC one, to stop the pipeline early in case of major issue (not able to compile/run a simple test)
Simple run:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Basic Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/trivial/


Privatization:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Basic Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/privatization/

#
## a regular test, to assess some basic MPC feature
#

MPI Simple C:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Regular Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/simple/c/

MPI Simple Fortran:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Regular Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/simple/fortran/

MPI NBC:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Regular Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/NBC/

Lowcomm:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Regular Testing
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/lowcomm/


#
# Benchmarks
#

MPI IMB-MPI 2017:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Benchmarks
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/IMB_2017/check-mpi/

MPI IMB-NBC 2017:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Benchmarks
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/MPI/IMB_2017/check-nbc/

#
# Applications
#


Lulesh:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Applications
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/corals/lulesh-2.0.3/

miniFe:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Applications
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/corals/miniFe/

NAS-MZ MPI:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Applications
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/hybrid/NAS/MZ-MPI/

NAS-MZ OMP:
  <<: *run_by_main
  <<: *uses_slurm
  stage: Applications
  script:
    - *pcvs_prepare
    - ${PCVS_COMMAND} $HOME/repo/mpc_ci/hybrid/NAS/MZ-OMP/


###################################
####### MANUALLY TRIGGERED ########
###################################
#This section gathers jobs not scheduled to be part of the standard pipeline.
#They could be run manually or periodically.

#When triggered through a variable, MPC is built in containers with fixed software stack (base compiler, libc, etc...)
# to ensure MPC to compile in a large variety of environments

Debian Stretch:
  stage: Regular Testing
  script: "docker run --rm -it paratoolsfrance/mpc-env:debian-stretch $HOME/docker/run_installmpc.sh"
  tags:
    - "docker"
  only:
    variables:
      - $MPC_CI_MODE == "multibuild"

Centos 7:
  stage: Regular Testing
  script: "docker run --rm -it paratoolsfrance/mpc-env:centos-7 $HOME/docker/run_installmpc.sh"
  tags:
    - "docker"
  only:
    variables:
      - $MPC_CI_MODE == "multibuild"
