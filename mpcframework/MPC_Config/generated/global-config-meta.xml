<?xml version="1.0"?>
<all>
<config name="MPC_Accelerators">
	<usertypes>
		<struct name="accl_cuda" doc="CUDA-specific configuration">
			<param name="enabled" type="bool" default="false" doc="Set to true to enable CUDA context-switch" />
		</struct>
		<struct name="accl_openacc" doc="OpenACC-specific configuration">
			<param name="enabled" type="bool" default="false" doc="Set to true to enable OpenACC in MPC" />
		</struct>
		<struct name="accl_opencl" doc="OpenCL-specific configuration">
			<param name="enabled" type="bool" default="false" doc="Set to true to enable OpenCL in MPC" />
		</struct>

		<struct name="accl" doc="Options for MPC Accelerators module.">
			<param name="enabled" type="bool" default="false" doc="Set to true to enable Accelerators support" />
			<param name="cuda" type="accl_cuda" doc="Define CUDA-specific configuration" />
			<param name="openacc" type="accl_openacc" doc="Define OpenACC-specific configuration" />
			<param name="opencl" type="accl_opencl" doc="Define OpenCL-specific configuration" />
		</struct>
	</usertypes>
	<modules>
		<module name="accelerator" type="accl"/>
	</modules>
</config>
<!--
############################# MPC License ##############################
# Wed Nov 19 15:19:19 CET 2008                                         #
# Copyright or (C) or Copr. Commissariat a l'Energie Atomique          #
#                                                                      #
# IDDN.FR.001.230040.000.S.P.2007.000.10000                            #
# This file is part of the MPC Runtime.                                #
#                                                                      #
# This software is governed by the CeCILL-C license under French law   #
# and abiding by the rules of distribution of free software.  You can  #
# use, modify and/ or redistribute the software under the terms of     #
# the CeCILL-C license as circulated by CEA, CNRS and INRIA at the     #
# following URL http://www.cecill.info.                                #
#                                                                      #
# The fact that you are presently reading this means that you have     #
# had knowledge of the CeCILL-C license and that you accept its        #
# terms.                                                               #
#                                                                      #
# Authors:                                                             #
#   - VALAT Sebastien sebastien.valat@cea.fr                           #
#                                                                      #
########################################################################
-->
<config name="MPC_Allocator">
	<usertypes>
		<struct name="allocator"             doc="Options for MPC memory allocator.">
			<!--
			                   !!!!!!  READ THIS BEFORE MODIFY !!!!!!!!!!

			!!!! CAUTION !!!!! Due to initialization steps, we must boostrap egg_allocator with default
			values, so allocator can only use options which can be override on-fly (during the run)
			and must not define fixed parameters.
			There is an exception for all steps which are done after egg_allocator init steps :
			    - NUMA setup and usage
				- Setup allocator of threads (memory source mapping, scope selection....)
			!!!! CAUTION !!!!! If you want to add an array here, you must fix function
			sctk_alloc_config_egg_init().
			!!!! CAUTION !!!!! Up to now default values are replicated into sctk_alloc_config_egg_init()
			don't forget to update it in case of change.
			-->
			<param name="numa_migration"     type="bool"    default="false"    doc="Enable or disable NUMA migration of allocator pages on thread migration."/>
			<param name="realloc_factor"     type="int"     default="2"        doc="If the new segment is less than N time smaller than factor, realloc will allocate a new segment, otherwise it will keep the same one. Use 1 to force realloc every time (may be slower but consume less memory)."/>
			<param name="realloc_threashold" type="size"    default="50MB"     doc="If the new segment is smaller of N bytes than threashold, realloc will allocate a new segment, otherwise it will keep the same one. Use 0 to force realloc every time (may be slower but consume less memory)."/>
			<param name="numa"               type="bool"    default="true"     doc="Permit to enable of disable NUMA support in MPC Allocator."/>
			<!--<param name="scope"              type="string"  default="thread"   doc="Define the scope the posix allocator, can be : process | vp | thread."/>-->
			<param name="strict"             type="bool"    default="false"    doc="If true, enable usage of abort() on free error, otherwise try to continue by skipping."/>
			<param name="keep_mem"           type="size"    default="500MB"     doc="Maximum amount of memory to keep in memory sources (one per NUMA node). Use 0 to disable cache, huge value to keep all."/>
			<param name="keep_max"           type="size"    default="8MB"      doc="Maximum size of macro blocs to keep in memory source for reuse. Use 0 to disable cache, huge value to keep all."/>
		</struct>
	</usertypes>
	<modules>
		<module name="allocator" type="allocator"/>
	</modules>
</config>
<config name="MPC_Common">
	<usertypes>
		<struct name="launcher" doc="Options for MPC launcher.">
			<param name="verbosity" type="int" default="0" doc="Default verbosity level from 0 to 3. Can be override by -vv on mpcrun." alias="LN_VERBOSITY" />
			<param name="banner" type="bool" default="true" doc="Display the MPC banner at launch time to print some informations about the topology. Can be override by MPC_DISABLE_BANNER." alias="LN_BANNER" />
			<param name="autokill" type="int" default="0" doc="Automatically kill the MPC processes after a given timeout. Use 0 to disable. Can be override by MPC_AUTO_KILL_TIMEOUT." alias="LN_AUTOKILL" />
			<param name="user_launchers" type="string" default="default" doc="Permit to extend the launchers available via 'mpcrun -l=...' by providing scripts (named mpcrun_XXXX) in a user directory. Can be override by MPC_USER_LAUNCHERS."/>
			<param name="keep_rand_addr" type="bool" default="true" doc="Activate randomization of base addresses" />
			<param name="disable_rand_addr" type="bool" default="false" doc="Deactivate randomization of base addresses" alias="LN_ADDR_RAND_DISABLE" />
			<param name="disable_mpc" type="bool" default="false" doc="Do not use mpc for execution (deprecated?)" alias="LN_DISABLE_MPC"/>
            <param name="thread_init" type="funcptr" doc="Initialize multithreading mode" default="sctk_use_ethread_mxn" />
			<param name="nb_task" type="int" default="1" doc="Define the number of MPI tasks" alias="LN_MPI_TASK" />
			<param name="nb_process" type="int" default="1" doc="Define the number of MPC processes" alias="LN_MPC_PROCESS" />
			<param name="nb_processor" type="int" default="0" doc="Define the number of virtual processors" alias="LN_MPC_VP" />
			<param name="nb_node" type="int" default="1" doc="Define the number of compute nodes" alias="LN_MPC_NODE" />
			<param name="launcher" type="string" default="none" doc="Define which launcher to use" />
			<param name="max_try" type="int" default="10" doc="Define the max number of tries to access the topology file before failing" alias="LN_MAX_TOPO_TRY" />
			<param name="vers_details" type="bool" default="false" doc="Print the MPC version number" alias="LN_DISPLAY_VERS" />
			<param name="profiling" type="string" default="stdout" doc="Select the type of outputs for the profiling" />
			<param name="enable_smt" type="bool" default="false" doc="Enable usage of hyperthreaded cores if available on current architecture." alias="LN_ENABLE_SMT" />
			<param name="share_node" type="bool" default="false" doc="Enable the restriction on CPU number to share node" alias="LN_SHARE_NODE" />
			<param name="restart" type="bool" default="false" doc="Restart MPC from a previous checkpoint" alias="LN_RESTART" />
			<param name="checkpoint" type="bool" default="false" doc="Enable MPC checkpointing" alias="LN_CHECKPOINT" />
			<param name="migration" type="bool" default="false" doc="Enable migration" alias="LN_MIGRATION" />
			<param name="report" type="bool" default="false" doc="Enable reporting." alias="LN_REPORT" />
		</struct>
		
		<struct name="debugger" doc="Options for MPC Debugger">
			<param name="colors" type="bool" default="true" doc="Print colored text in terminal"/>
			<param name="max_filename_size" type="int" default="1024" doc=""/>
			<param name="mpc_bt_sig" type="int" default="1" doc="Should MPC capture common signals also connected to the MPC_BT_SIG environment variable which supersedes the config" />
		</struct>
	</usertypes>
	<modules>
		<module name="launcher" type="launcher"/>
		<module name="debugger" type="debugger"/>
	</modules>
</config>
<config name="MPC_Fault_Tolerance">
	<usertypes>
		<struct name="ft" doc="Options for MPC Fault-Tolerance module.">
			<param name="enabled" type="bool" default="false" doc="Set to true to enable Fault-Tolerance support" />
		</struct>
	</usertypes>
	<modules>
		<module name="ft_system" type="ft"/>
	</modules>
</config>
<config name="MPC_MPI">
    <usertypes>
        <struct name="collectives_shm_shared" doc="Shared Memory Collectives for MPC">
            <!-- Function Pointers -->
            <param name="barrier_intra_shared_node" type="funcptr" default="__INTERNAL__PMPI_Barrier_intra_shared_node" doc="MPI_Barrier intracom algorithm on shared-node comms"/>
            <param name="bcast_intra_shared_node" type="funcptr" default="__INTERNAL__PMPI_Bcast_intra_shared_node" doc="MPI_Bcast intracom algorithm on shared-node comms"/>
            <param name="alltoall_intra_shared_node" type="funcptr" default="__INTERNAL__PMPI_Alltoall_intra_shared_node" doc="MPI_Alltoall intracom algorithm on shared-node comms"/>
            <param name="scatter_intra_shared_node" type="funcptr" default="__INTERNAL__PMPI_Scatter_intra_shared_node" doc="MPI_Scatter intracom algorithm on shared-node comms"/>
        </struct>
        <struct name="collectives_shm" doc="Shared Memory Collectives for MPC">
            <!-- Function Pointers -->
            <param name="barrier_intra_shm" type="funcptr" default="__INTERNAL__PMPI_Barrier_intra_shm" doc="MPI_Barrier intracom algorithm on shared communicators"/>
            <param name="bcast_intra_shm" type="funcptr" default="__INTERNAL__PMPI_Bcast_intra_shm" doc="Type of MPI_Bcast intracom algorithm on shared communicators"/>
            <param name="alltoallv_intra_shm" type="funcptr" default="__INTERNAL__PMPI_Alltoallv_intra_shm" doc="Alltoallv intracom algorithm"/>
            <param name="gatherv_intra_shm" type="funcptr" default="__INTERNAL__PMPI_Gatherv_intra_shm" doc="MPI_Gatherv intracom algorithm for shared communicators"/>
            <param name="scatterv_intra_shm" type="funcptr" default="__INTERNAL__PMPI_Scatterv_intra_shm" doc="MPI_Scatterv intracom algorithm on shared communicators"/>
            <param name="reduce_intra_shm" type="funcptr" default="__INTERNAL__PMPI_Reduce_shm" doc="MPI_Reduce intracom shared-mem algorithm"/>

            <!-- Global Parameters -->	
            <param name="topo_tree_arity" type="int" default="-1" doc="Arrity being used to build topological communicators  '-1' means auto-compute to match processes and NUMA" alias="MPI_COLL_TOPO_TREE_ARITY"/>
            <param name="topo_tree_dump" type="bool" default="false" doc="Dump topological comm tree in DOT (fname topoN.cdat) with N the communicator size" alias="MPI_COLL_TOPO_TREE_DUMP"/>
            <param name="coll_force_nocommute" type="bool" default="false" doc="Force the use of deterministic algorithms" alias="MPI_COLL_NO_COMMUTE" />
            <!-- MPI_Reduce -->	

            <param name="reduce_pipelined_blocks" type="int" default="16" doc="Number of blocks for pipelined Reduce" alias="MPI_COLL_REDUCE_PIPELINED_BLOCKS"/>
            <param name="reduce_pipelined_tresh" type="size" default="1KB" doc="Size required to rely on pipelined reduce" alias="MPI_COLL_REDUCE_INTERLEAVE_TRSH"/>
            <param name="reduce_interleave" type="int" default="8" doc="Number of reduce slots to allocate (required to be power of 2)" alias="MPI_COLL_REDUCE_INTERLEAVE"/>

            <!-- MPI_Bcast -->	
            <param name="bcast_interleave" type="int" default="8" doc="Number of bcast slots to allocate (required to be power of 2)" alias="MPI_COLL_BCAST_INTERLEAVE"/>
        </struct>


        <struct name="collectives_intra" doc="Collectives intracom MPI">
            <param name="barrier_intra" type="funcptr" default="__INTERNAL__PMPI_Barrier_intra" doc="MPI_Barrier intracom algorithm"/>
			<param name="barrier_intra_for_trsh" type="int" default="33" doc="Maximum number of process for using a trivial for for the Barrier" alias="MPI_COLL_BARRIER_FOR_TRSH"/>
			<param name="bcast_intra" type="funcptr" default="__INTERNAL__PMPI_Bcast_intra" doc="Type of MPI_Bcast intracom algorithm"/>
			<param name="bcast_intra_for_trsh" type="int" default="33" doc="Maximum number of process for using a trivial for for the Bcast" alias="MPI_COLL_BCAST_FOR_TRSH"/>
			<param name="bcast_intra_for_count_trsh" type="int" default="1024" doc="Maximum number of elems for using a trivial for for the Bcast" alias="MPI_COLL_BCAST_FOR_ELEM_TRSH"/>

            <param name="allgather_intra" type="funcptr" default="__INTERNAL__PMPI_Allgather_intra" doc="MPI_Allgather intracom algorithm"/>
            <param name="allgatherv_intra" type="funcptr" default="__INTERNAL__PMPI_Allgatherv_intra" doc="MPI_Allgatherv intracom algorithm"/>
            <param name="alltoall_intra" type="funcptr" default="__INTERNAL__PMPI_Alltoall_intra" doc="MPI_Alltoall intracom algorithm"/>
            <param name="alltoallv_intra" type="funcptr" default="__INTERNAL__PMPI_Alltoallv_intra" doc="Alltoallv intracom algorithm"/>
            <param name="alltoallw_intra" type="funcptr" default="__INTERNAL__PMPI_Alltoallw_intra" doc="MPI_Alltoallw intracom algorithm"/>
            <param name="gather_intra" type="funcptr" default="__INTERNAL__PMPI_Gather_intra" doc="MPI_Gather intracom algorithm"/>
            <param name="gatherv_intra" type="funcptr" default="__INTERNAL__PMPI_Gatherv_intra" doc="MPI_Gatherv intracom algorithm"/>
            <param name="scatter_intra" type="funcptr" default="__INTERNAL__PMPI_Scatter_intra" doc="MPI_Scatter intracom algorithm"/>
            <param name="scatterv_intra" type="funcptr" default="__INTERNAL__PMPI_Scatterv_intra" doc="MPI_Scatterv intracom algorithm"/>
            <param name="scan_intra" type="funcptr" default="__INTERNAL__PMPI_Scan_intra" doc="MPI_Scan intracom algorithm"/>
            <param name="exscan_intra" type="funcptr" default="__INTERNAL__PMPI_Exscan_intra" doc="MPI_Exscan intracom algorithm"/>
			
			<param name="reduce_intra" type="funcptr" default="__INTERNAL__PMPI_Reduce_intra" doc="MPI_Reduce intracom algorithm"/>
			<param name="reduce_intra_for_trsh" type="int" default="33" doc="Maximum number of process for using a trivial for for the Reduce" alias="MPI_COLL_REDUCE_FOR_TRSH"/>
			<param name="reduce_intra_for_count_trsh" type="int" default="1024" doc="Maximum number of elements for using a trivial for for the Reduce" alias="MPI_COLL_REDUCE_FOR_ELEM_TRSH"/>
			
			<param name="allreduce_intra" type="funcptr" default="__INTERNAL__PMPI_Allreduce_intra" doc="MPI_Allreduce intracom algorithm"/>
			
			
			<param name="reduce_scatter_intra" type="funcptr" default="__INTERNAL__PMPI_Reduce_scatter_intra" doc="MPI_Reduce_scatter intracom algorithm"/>
            <param name="reduce_scatter_block_intra" type="funcptr" default="__INTERNAL__PMPI_Reduce_scatter_block_intra" doc="MPI_Reduce_scatter_block intracom algorithm"/>
        </struct>

        <struct name="collectives_inter" doc="Collectives intercom MPI">
            <param name="barrier_inter" type="funcptr" default="__INTERNAL__PMPI_Barrier_inter" doc="MPI_Barrier intercom algorithm"/>
            <param name="bcast_inter" type="funcptr" default="__INTERNAL__PMPI_Bcast_inter" doc="MPI_Barrier intercom algorithm"/>
            <param name="allgather_inter" type="funcptr" default="__INTERNAL__PMPI_Allgather_inter" doc="MPI_Allgather intercom algorithm"/>
            <param name="allgatherv_inter" type="funcptr" default="__INTERNAL__PMPI_Allgatherv_inter" doc="MPI_Allgatherv intercom algorithm"/>
            <param name="alltoall_inter" type="funcptr" default="__INTERNAL__PMPI_Alltoall_inter" doc="MPI_Alltoall intercom algorithm"/>
            <param name="alltoallv_inter" type="funcptr" default="__INTERNAL__PMPI_Alltoallv_inter" doc="MPI_Alltoallv intercom algorithm"/>
            <param name="alltoallw_inter" type="funcptr" default="__INTERNAL__PMPI_Alltoallw_inter" doc="MPI_Alltoallw intercom algorithm"/>
            <param name="gather_inter" type="funcptr" default="__INTERNAL__PMPI_Gather_inter" doc="MPI_Gather intercom algorithm"/>
            <param name="gatherv_inter" type="funcptr" default="__INTERNAL__PMPI_Gatherv_inter" doc="MPI_Gatherv intercom algorithm"/>
            <param name="scatter_inter" type="funcptr" default="__INTERNAL__PMPI_Scatter_inter" doc="MPI_Scatter intercom algorithm"/>
            <param name="scatterv_inter" type="funcptr" default="__INTERNAL__PMPI_Scatterv_inter" doc="MPI_Scatterv intercom algorithm"/>
            <param name="reduce_inter" type="funcptr" default="__INTERNAL__PMPI_Reduce_inter" doc="MPI_Reduce intercom algorithm"/>
            <param name="allreduce_inter" type="funcptr" default="__INTERNAL__PMPI_Allreduce_inter" doc="MPI_Allreduce intercom algorithm"/>
            <param name="reduce_scatter_inter" type="funcptr" default="__INTERNAL__PMPI_Reduce_scatter_inter" doc="MPI_Reduce_scatter intercom algorithm"/>
            <param name="reduce_scatter_block_inter" type="funcptr" default="__INTERNAL__PMPI_Reduce_scatter_block_inter" doc="MPI_Reduce_scatter_block intercom algorithm"/>
        </struct>

        <struct name="nbc" doc="NBC">
            <param name="use_progress_thread" type="int" default="0" doc="If use progress threads for non blocking collectives"/>
            <param name="progress_thread_binding" type="funcptr" default="sctk_get_progress_thread_binding_bind" doc="Algorithm of progress threads binding : sctk_get_progress_thread_binding_[bind,smart,numa_iter,numa]"/>
            <param name="use_egreq_bcast" type="int" default="0" doc="Should bcast rely on Egreq progress"/>
            <param name="use_egreq_scatter" type="int" default="0" doc="Should scatter rely on Egreq progress"/>
            <param name="use_egreq_gather" type="int" default="0" doc="Should gather rely on Egreq progress"/>
            <param name="use_egreq_reduce" type="int" default="0" doc="Should reduce rely on Egreq progress"/>
            <param name="use_egreq_barrier" type="int" default="0" doc="Should barrier rely on Egreq progress"/>
        </struct>
        <!--
        <struct name="scheduler" doc="Scheduler priority parameters">
            <param name="sched_NBC_Pthread_basic_priority" type="int" default="10" doc="Basic priority of polling tasks"/>
        </struct>
        -->

                <!-- ############################## -->
                <!-- #    MPI RMA OPTIONS         # -->
                <!-- ############################## -->

                <struct name="mpi_rma" doc="Options related to the MPI RMA support">
                    <param name="alloc_mem_pool_enable" type="int" default="1" doc="Enable the MPI_Alloc_mem shared memory pool"/>
                    <param name="alloc_mem_pool_size" type="size" default="1MB" doc="Size of the MPI_Alloc_mem pool"/>
                    <param name="alloc_mem_pool_autodetect" type="int" default="1" doc="Alloc the MPI_Alloc_mem pool to grow linear for some apps"/>
                    <param name="alloc_mem_pool_force_process_linear" type="int" default="0" doc="Force the size to be a quantum per local process"/>
			<param name="alloc_mem_pool_per_process_size" type="size" default="1MB" doc="Quantum to allocate to each process when linear forced"/>
                    <param name="win_thread_pool_max" type="int" default="2" doc="Maximum number of window threads to keep"/>
                </struct>


                <!-- ############################## -->
                <!-- #             MPC            # -->
                <!-- ############################## -->

                <struct name="mpc" doc="Options for MPC Message Passing">
                    <param name="log_debug" type="bool" default="false" doc="Print debug messages"/>
                    <param name="hard_checking" type="bool" default="false" doc=""/>
                    <param name="buffering" type="bool" default="false" doc=""/>
                </struct>


            </usertypes>


            <modules>
                <module name="collectives_shm_shared" type="collectives_shm_shared"/>
                <module name="collectives_shm" type="collectives_shm"/>
                <module name="collectives_intra" type="collectives_intra"/>
                <module name="collectives_inter" type="collectives_inter"/>
                <module name="nbc" type="nbc"/>
                <module name="mpc" type="mpc"/>
                <module name="rma" type="mpi_rma"/>
            </modules>



        </config>
<config name="MPC_Message_Passing">
	<usertypes>

		<!-- ############################## -->
		<!-- #           DRIVERS          # -->
		<!-- ############################## -->
		<struct name="net_driver_topological" doc="Declare a topological driver.">
			<param name="dummy" type="int" doc="A test Param"/>
		</struct>


		<!-- Structure for infiniband driver -->
		<struct name="net_driver_infiniband" doc="Declare a fake driver to test the configuration system.">
			<param name="pkey" type="string" doc="Define the pkey value" default="undefined"/>
			<param name="adm_port" type="int" doc="Defines the port number to use." default="1" />
			<param name="verbose_level" type="int" doc="Defines the verbose level of the Infiniband interface ." default="0" alias="IB_VERBOSE" />
			<param name="eager_limit" type="int" doc="Size of the eager buffers (short messages)." default="12288" alias="IB_EAGER_THRESH" />
			<param name="buffered_limit" type="int" doc="Max size for using the Buffered protocol (message split into several Eager messages)." default="262114" alias="IB_BUFFERED_THRESH" />
			<param name="qp_tx_depth" type="int" doc="Number of entries to allocate in the QP for sending messages. If too low, may cause an QP overrun" default="15000" alias="IB_TX_DEPTH" />
			<param name="qp_rx_depth" type="int" doc="Number of entries to allocate in the QP for receiving messages. Must be 0 if using SRQ" default="0" alias="IB_RX_DEPTH" />
			<param name="cq_depth" type="int" doc="Number of entries to allocate in the CQ. If too low, may cause a CQ overrun" default="40000" alias="IB_CQ_DEPTH" />
			<param name="rdma_depth" type="int" doc="Number of RDMA resources on QP (covers both max_dest_rd_atomic and max_rd_atomic)" default="16" alias="IB_RDMA_DEPTH" />
			<param name="max_sg_sq" type="int" doc="Max pending RDMA operations for send" default="4" alias="IB_MAX_SG_SQ" />
			<param name="max_sg_rq" type="int" doc="Max pending RDMA operations for recv" default="4" alias="IB_MAX_SG_RQ" />
			<param name="max_inline" type="int" doc="Max size for inlining messages" default="128" alias="IB_MAX_INLINE" />
			<param name="rdma_resizing" type="int" doc="Defines if RDMA connections may be resized." default="0" alias="IB_RDMA_RESIZING" />
			<param name="max_rdma_connections" type="int" doc="Number of RDMA buffers allocated for each neighbor" default="0" alias="IB_RDMA_CONN" />
			<param name="max_rdma_resizing" type="int" doc="Max number of RDMA buffers resizing allowed" default="0" alias="IB_RDMA_MAX_RESIZING" />
			<param name="init_ibufs" type="int" doc="Max number of Eager buffers to allocate during the initialization step" default="1000" alias="IB_INIT_IBUF" />
   			<param name="init_recv_ibufs" type="int" doc="Defines the number of receive buffers initially allocated. The number is on-the-fly expanded when needed (see init_recv_ibufs_chunk)" default="200" alias="IB_INIT_RECV_IBUF" />
			<param name="max_srq_ibufs_posted" type="int" doc="Max number of Eager buffers which can be posted to the SRQ. This number cannot be higher than the number fixed by the HW" default="1500" alias="IB_MAX_SRQ_IBUF_POSTED" />
			<param name="max_srq_ibufs" type="int" doc="Max number of Eager buffers which can be used by the SRQ. This number is not fixed by the HW" default="1000" alias="IB_MAX_SRQ_IBUF" />
			<param name="srq_credit_limit" type="int" doc="Min number of free recv Eager buffers before posting a new buffer." default="500" alias="IB_SRQ_CREDIT_LIMIT" />
			<param name="srq_credit_thread_limit" type="int" doc="Min number of free recv Eager buffers before the activation of the asynchronous thread. If this thread is activated too many times, the performance may be decreased." default="100" alias="IB_SRQ_CREDIT_LIMIT_THREAD"/>
			<param name="size_ibufs_chunk" type="int" doc="Number of new buffers allocated when no more buffers are available." default="100" alias="IB_IBUF_CHUNK" />
			<param name="init_mr" type="int" doc="Number of MMU entries allocated during the MPC initlization." default="400" alias="IB_MMU_INIT" />
			<param name="steal" type="int" doc="Defines if the steal in MPI is allowed " default="2" alias="IB_CAN_STEAL" />
			<param name="quiet_crash" type="int" doc="Defines if the Infiniband interface must crash quietly." default="0" alias="IB_QUIET_CRASH" />
			<param name="async_thread" type="int" doc="Defines if the asynchronous may be started at the MPC initialization." default="0" alias="IB_ASYNC_THREAD" />
			<param name="wc_in_number" type="int" doc="Defines the number of entries for the CQ dedicated to received messages." default="0" alias="IB_WC_IN" />
			<param name="wc_out_number" type="int" doc="Defines the number of entries for the CQ dedicated to sent messages." default="0" alias="IB_WC_OUT" />
			<param name="low_memory" type="bool" doc="Defines if the low memory mode should be activated" default="false" />
			<param name="rdvz_protocol" type="ibv_rdvz_protocol" doc="Defines the Rendezvous protocol to use (IBV_RDVZ_WRITE_PROTOCOL or IBV_RDVZ_READ_PROTOCOL)" default="IBV_RDVZ_WRITE_PROTOCOL" />
			<param name="rdma_min_size" type="int" doc="Defines the minimum size for the Eager RDMA buffers" default="1024" />
			<param name="rdma_max_size" type="int" doc="Defines the maximun size for the Eager RDMA buffers" default="4096" />
			<param name="rdma_min_nb" type="int" doc="Defines the minimum number of Eager RDMA buffers" default="8" />
			<param name="rdma_max_nb" type="int" doc="Defines the maximum number of Eager RDMA buffers" default="32" />
			<param name="rdma_resizing_min_size" type="int" doc="Defines the minimum size for the Eager RDMA buffers (resizing)" default="1024" />
			<param name="rdma_resizing_max_size" type="int" doc="Defines the maximum size for the Eager RDMA buffers (resizing)" default="4096" />
			<param name="rdma_resizing_min_nb" type="int" doc="Defines the minimum number of Eager RDMA buffers (resizing)" default="8" />
			<param name="rdma_resizing_max_nb" type="int" doc="Defines the maximum number of Eager RDMA buffers (resizing)" default="32" />
			<param name="size_recv_ibufs_chunk" type="int" doc="Defines the number of receive buffers allocated on the fly." default="400" alias="IB_RECV_IBUF_CHUNK" />
		</struct>

		<struct name="ib_global" doc="Global Parameters for IB common structs.">
			<param name="mmu_cache_enabled" type="int" doc="Defines if the MMU cache is enabled." default="1" alias="IB_MMU_CHACHE_ENABLED" />
			<param name="mmu_cache_entry_count" type="int" doc="Number of entries to keep in the cache." default="1000" alias="IB_MMU_CHACHE_COUNT" />
			<param name="mmu_cache_maximum_size" type="size" doc="Total size of entries to keep in the cache." default="4GB"  alias="IB_MMU_CHACHE_MAX_SIZE" />
			<param name="mmu_cache_maximum_pin_size" type="size" doc="Maximum size of an pinned entry." default="1GB" alias="IB_MMU_CHACHE_MAX_PIN_SIZE" />
		</struct>

		<enum name="ibv_rdvz_protocol" doc="">
			<value>IBV_RDVZ_WRITE_PROTOCOL</value>
			<value>IBV_RDVZ_READ_PROTOCOL</value>
		</enum>

		<!-- Structure for Portals driver -->
		<struct name="net_driver_portals" doc="Portals-based driver">
			<param name="eager_limit" type="size" doc="Max size of messages allowed to use the eager protocol." default="8 KB" />
			<param name="min_comms" type="int" doc="Min number of communicators (help to avoid dynamic PT entry allocation)" default="1" />
			<param name="block_cut" type="size" doc="Above this value, RDV messages will be split in multiple GET requests" default="2 GB" />
		</struct>

		<!-- Structure for TCP driver -->
		<struct name="net_driver_tcp" doc="TCP-based driver">
			<param name="tcpoib" type="int" doc="Enable TCP over Infiniband (if elligible)." default="1" />
		</struct>

		<!-- Structure for TCP RDMA driver -->
		<struct name="net_driver_tcp_rdma" doc="TCP-Based RDMA implementation">
			<param name="tcpoib" type="int" doc="Enable TCP over Infiniband (if elligible)." default="1" />
		</struct>

		<!-- Structure for Shared Memory driver -->
        <struct name="net_driver_shm" doc="Inter-Process shared memory communication implementation">
                    <param name="buffered_priority" type="int" doc="Defines priority for the SHM buffered message" default="0" alias="SHM_BUFF_PRIORITY" />
                    <param name="buffered_min_size" type="int" doc="Defines the min size for the SHM buffered message" default="0" alias="SHM_BUFF_MIN_SIZE" />
                    <param name="buffered_max_size" type="int" doc="Defines the min size for the SHM buffered message" default="4096" alias="SHM_BUFF_MAX_SIZE" />
                    <param name="buffered_zerocopy" type="bool" doc="Defines if mode zerocopy should be actived for SHM buffered message" default="false" alias="SHM_BUFF_ZEROCOPY" />
                    <param name="cma_enable" type="bool" doc="" default="true" alias="SHM_CMA_ENABLE"/>
                    <param name="cma_priority" type="int" doc="Defines priority for the SHM CMA message" default="1" alias="SHM_CMA_PRIORITY" />
                    <param name="cma_min_size" type="int" doc="Defines the min size for the SHM CMA message" default="4096"  alias="SHM_CMA_MIN_SIZE" />
                    <param name="cma_max_size" type="int" doc="Defines the min size for the SHM CMA message" default="0" alias="SHM_CMA_MAX_SIZE" />
                    <param name="cma_zerocopy" type="bool" doc="Defines if mode zerocopy should be actived for SHM CMA message" default="false" alias="SHM_CMA_ZEROCOPY" />
                    <param name="frag_priority" type="int" doc="Defines priority for the SHM fragmented message" default="2" alias="SHM_FRAG_PRIORITY" />
                    <param name="frag_min_size" type="int" doc="Defines the min size for the SHM fragmented message" default="4096" alias="SHM_FRAG_MIN_SIZE" />
                    <param name="frag_max_size" type="int" doc="Defines the min size for the SHM fragmented message" default="0" alias="SHM_FRAG_MAX_SIZE" />
                    <param name="frag_zerocopy" type="bool" doc="Defines if mode zerocopy should be actived for SHM fragmented message" default="false" alias="SHM_FRAG_ZEROCOPY" />
            		<param name="shmem_size" type="int" doc="Size of shared memory region." default="1024" alias="SHM_SIZE" />
            		<param name="cells_num" type="int" doc="Size of shared memory region." default="2048" alias="SHM_CELLS" />
        	</struct>

		<!-- Union to merge drivers -->
		<union name="net_driver" doc="Define a specific configuration for a network driver to apply in rails.">
			<choice name="infiniband" type="net_driver_infiniband" />
			<choice name="portals" type="net_driver_portals" />
			<choice name="tcp" type="net_driver_tcp" />
			<choice name="tcprdma" type="net_driver_tcp_rdma" />
			<choice name="shm" type="net_driver_shm" />
			<choice name="topological" type="net_driver_topological" />
		</union>

		<!-- ############################## -->
		<!-- #      DRIVERS  CONFIG       # -->
		<!-- ############################## -->

		<!-- define a network driver config -->
		<struct name="net_driver_config" doc="Contain a list of driver configuration reused by rail definitions.">
			<param name="name" type="string" doc="Name of the driver configuration to be referenced in rail definitions." />
			<param name="driver" type="net_driver" doc="Define the related driver to use and its configuration." />
		</struct>

		<!-- ############################## -->
		<!-- #         RAIL GATES         # -->
		<!-- ############################## -->

		<!-- Rail Gates -->

		<struct name="gate_boolean" doc="This gate applies given thruth value to messages.">
			<param name="value" type="int" doc="whereas to accept input messages or not" default="1" />
			<param name="gatefunc" type="funcptr" doc="Function to be called for this gate" default="sctk_rail_gate_boolean"/>
		</struct>

		<struct name="gate_probabilistic" doc="This gate uses a given rail with a parametrized probability.">
			<param name="probability" type="int" doc="Probability to choose this rail in percents (ralatively to this single rail, integer)" default="50" />
			<param name="gatefunc" type="funcptr" default="sctk_rail_gate_probabilistic" doc="Function to be called for this gate"/>
		</struct>

		<struct name="gate_min_size" doc="This gate uses a given rail if size is at least a given value.">
			<param name="value" type="size" doc="Minimum size to choose this rail (with units)"/>
			<param name="gatefunc" type="funcptr" default="sctk_rail_gate_minsize" doc="Function to be called for this gate"/>
		</struct>

		<struct name="gate_max_size" doc="This gate uses a given rail if size is at most a given value.">
			<param name="value" type="size" doc="Maximum size to choose this rail (with units)"/>
			<param name="gatefunc" type="funcptr" default="sctk_rail_gate_maxsize" doc="Function to be called for this gate"/>
		</struct>

		<struct name="gate_message_type" doc="This gate can be used define which type of message can use a given rail.">
			<param name="process" type="int" doc="Process Specific Messages can use this rail" default="1" />
			<param name="task" type="int" doc="Task specific messages can use this rail" default="1" />
			<param name="emulated_rma" type="int" doc="Task specific messages can use this rail" default="1" />
			<param name="common" type="int" doc="Common messages (MPI) can use this rail" default="1" />
			<param name="gatefunc" type="funcptr" default="sctk_rail_gate_msgtype" doc="Function to be called for this gate"/>
		</struct>

		<struct name="gate_user" doc="This gate uses a given rail with a user defined function.">
			<param name="gatefunc" type="funcptr" default="sctk_rail_gate_true" doc="Function to be called for this gate"/>
		</struct>

		<!-- Union to merge gates -->
		<union name="net_gate" doc="Defines gates and their configuration.">
			<choice name="boolean" type="gate_boolean" />
			<choice name="probabilistic" type="gate_probabilistic" />
			<choice name="minsize" type="gate_min_size" />
			<choice name="maxsize" type="gate_max_size" />
			<choice name="msgtype" type="gate_message_type" />
			<choice name="user" type="gate_probabilistic" />
		</union>


		<!-- ############################## -->
		<!-- #          RAIL              # -->
		<!-- ############################## -->

		<enum name="rail_topological_polling_level" doc="Values used for topological polling in the rail configuration">
			<value>RAIL_POLL_NONE</value>
			<value>RAIL_POLL_PU</value>
			<value>RAIL_POLL_CORE</value>
			<value>RAIL_POLL_SOCKET</value>
			<value>RAIL_POLL_NUMA</value>
			<value>RAIL_POLL_MACHINE</value>
		</enum>

		<struct name="topological_polling" doc="Defines a topological polling configuration.">
			<param name="range" type="rail_topological_polling_level" doc="Define the subset of cores involved in the polling." default="RAIL_POLL_MACHINE"/>
			<param name="trigger" type="rail_topological_polling_level" doc="Define the subset of cores involved in the polling." default="RAIL_POLL_SOCKET"/>
		</struct>

		<!-- Structure for rail definition -->
		<struct name="net_rail" doc="Define a rail which is a name, a device associate to a driver and a routing topology.">
			<param name="name" type="string" doc="Define the name of current rail." />
			<param name="priority" type="int" doc="Number which defines the order in which routes are tested (higher first)." default="1"  />
			<param name="device" type="string" doc="Define the name of the device to use in this rail." default="default"/>
			<param name="idle_polling" type="topological_polling" doc="Define how the idle polling is done." />
			<param name="any_source_polling" type="topological_polling" doc="Define how the any-source polling is done." />
			<param name="topology" type="string" doc="Define the network topology to apply on this rail." default="ring"/>
			<param name="ondemand" type="int" doc="Define if on-demand connections are allowed on this rail."  default="1"/>
			<param name="rdma" type="int" doc="Defines if the rail has RDMA enabled."  default="0"/>
			<param name="config" type="string" doc="Define the driver config to use for this rail." default="topological" />
			<array name="gates" entry-name="gate" type="net_gate" doc="List of gates to be applied in this config." />
			<array name="subrails" entry-name="subrail" type="net_rail" doc="Used for topological rail selection" />
		</struct>

		<!-- ############################## -->
		<!-- #    COMMAND LINE OPTION     # -->
		<!-- ############################## -->

		<!-- Structure for cli_option -->
		<struct name="net_cli_option" doc="Define a specific configuration for a network provided by '-net'.">
			<param name="name" type="string" doc="Define the name of the option." />
			<array name="rails" entry-name="rail" type="string" doc="Define the driver config to use for this rail." />
		</struct>

		<!-- root structure of network config -->
		<struct name="networks" doc="Base structure to contain the network configuration">
			<array name="configs" entry-name="config" type="net_driver_config" doc="Define the configuration driver list to reuse in rail definitions." />
			<array name="rails" entry-name="rail" type="net_rail" doc="List of rails to declare in MPC." />
			<array name="cli_options" entry-name="cli_option" type="net_cli_option" doc="List of networks available through the '-net' argument of mpcrun." />
		</struct>

		<!-- ############################## -->
		<!-- #    INTER THREAD COMM       # -->
		<!-- ############################## -->

		<struct name="inter_thread_comm" doc="Options for communication between threads">
			<param name="barrier_arity" type="int" default="8" doc="" alias="ITT_BARRIER_ARITY"/>
			<param name="broadcast_arity_max" type="int" default="32" doc="" alias="ITT_BROADCAST_ARITY"/>
			<param name="broadcast_max_size" type="int" default="1024" doc="" alias="ITT_BROADCAST_MAX_SIZE"/>
			<param name="broadcast_check_threshold" type="int" default="512" doc="" alias="ITT_BROADCAST_CHECK_TH"/>
			<param name="allreduce_arity_max" type="int" default="8" doc="" alias="ITT_ALLREDUCE_ARITY_MAX"/>
			<param name="allreduce_max_size" type="int" default="4096" doc="" alias="ITT_ALLREDUCE_MAX_SIZE"/>
			<param name="allreduce_check_threshold" type="int" default="8192" doc="" alias="ITT_ALLREDUCE_CHECK_TH"/>
			<param name="ALLREDUCE_MAX_SLOT" type="int" default="65536" doc="Slot size for allreduce" alias="ITT_ALLREDUCE_MAX_SLOT"/>
			<param name="collectives_init_hook" type="funcptr" default="sctk_collectives_init_opt_noalloc_split_messages" doc=""/>
		</struct>

		<!-- ############################## -->
		<!-- #    LOW LEVEL COMM          # -->
		<!-- ############################## -->

		<struct name="low_level_comm" doc="">
			<param name="checksum" type="bool" default="true" doc=""/>
			<param name="send_msg" type="funcptr" default="sctk_network_send_message_default" doc=""/>
			<param name="network_mode" type="string" default="default" doc=""/>
			<param name="dyn_reordering" type="bool" default="false" doc=""/>
			<param name="enable_idle_polling" type="bool" default="false" doc="Enable usage of polling during idle."/>
			<param name="ib_global" type="ib_global" doc="Global parameters for IB" />
		</struct>

	</usertypes>
	<modules>
		<module name="inter_thread_comm" type="inter_thread_comm" />
		<module name="low_level_comm" type="low_level_comm" />
	</modules>
</config>
<config name="MPC_OpenMP">
    <usertypes>

        <enum name="mpcomp_task_larceny_mode_t" doc="">
            <value>MPCOMP_TASK_LARCENY_MODE_HIERARCHICAL</value>
            <value>MPCOMP_TASK_LARCENY_MODE_RANDOM</value>
            <value>MPCOMP_TASK_LARCENY_MODE_RANDOM_ORDER</value>
            <value>MPCOMP_TASK_LARCENY_MODE_ROUNDROBIN</value>
            <value>MPCOMP_TASK_LARCENY_MODE_PRODUCER</value>
            <value>MPCOMP_TASK_LARCENY_MODE_PRODUCER_ORDER</value>
            <value>MPCOMP_TASK_LARCENY_MODE_COUNT</value>
        </enum>

            <struct name="openmp" doc="Options for MPC OpenMP.">

                <param name="vp" type="int" default="0" doc="Number of VPs for each OpenMP team"/>
                <param name="schedule" type="string" default="static" doc="Runtime schedule type and chunck size"/>
                <param name="nb_threads" type="int" doc="Number of threads to use during execution"/>
                <param name="adjustment" type="bool" default="false" doc="Dynamic adjustment of the number of threads"/>
                <param name="proc_bind" type="bool" default="true" doc="Bind threads to processor core"/>
                <param name="nested" type="bool" default="false" doc="Nested parallelism"/>
                <param name="stack_size" type="int" default="0" doc="Stack size for OpenMP threads"/>
                <param name="wait_policy" type="int" default="0" doc="Behavior of threads while waiting"/>
                <param name="thread_limit" type="int" default="0" doc="Maximum number of OpenMP threads among all teams"/>
                <param name="max_active_levels" type="int" default="0" doc="Maximum depth of nested parallelism"/>
                <param name="tree" type="string" default="" doc="Tree shape for OpenMP construct"/>
                <param name="max_threads" type="int" default="64" doc="Maximum number of threads for each team of a parallel region"/>
                <param name="max_alive_for_dyn" type="int" default="7" doc="Maximum number of shared for loops w/ dynamic schedule alive"/>
                <param name="max_alive_for_guided" type="int" default="3" doc="Maximum number of shared for loops w/ guided schedule alive"/>
                <param name="max_alive_sections" type="int" default="3" doc="Maximum number of alive sections construct"/>
                <param name="max_alive_single" type="int" default="3" doc="Maximum number of alive single construct"/>
                <param name="warn_nested" type="bool" default="false" doc="Emit warning when entering nested parallelism"/>
                <param name="mode" type="string" default="simple-mixed" doc="MPI/OpenMP hybrid mode (simple-mixed, alternating)"/>
                <param name="affinity" type="string" default="balanced" doc="Affinity of threads for parallel regions (COMPACT, SCATTER, BALANCED)"/>
			
				<!--  task static variables -->	
				<param name="omp_new_task_depth" 		type="int" default="0"	doc="Depth of the new tasks lists in the tree" />
				<param name="omp_untied_task_depth" 	type="int" default="0"  doc="Depth of the untied tasks lists in the tree" />
				<param name="omp_task_larceny_mode" 	type="mpcomp_task_larceny_mode_t" default="MPCOMP_TASK_LARCENY_MODE_HIERARCHICAL" doc="Task stealing policy"/>
				<param name="omp_task_nesting_max" 	type="int" default="8" doc="Task max depth in task generation" />
				<param name="mpcomp_task_max_delayed"  type="int" default="1024" doc=" Max tasks in mpcomp list" />

				<!--  OpenMP places -->	
                <param name="places" type="string" default="cores" doc="OpenMP places"/>
					
            </struct>

            <!-- </enum> -->
    </usertypes>
    <modules>
        <module name="openmp" type="openmp"/>
    </modules>
</config>
<config name="MPC_Profiler">
	<usertypes>
		<struct name="profiler"     doc="Options for the internal MPC Profiler">
			<param name="file_prefix"   type="string" default="mpc_profile" doc="Prefix of MPC Profiler outputs"/>
			<param name="append_date"   type="bool" default="true" doc="Add a timestamp to profiles file names"/>
			<param name="color_stdout"   type="bool" default="true" doc="Profile in color when outputed to stdout"/>
			
			<array name="level_colors" entry-name="level" type="string" doc="Color for levels of profiler output">
				<default>
					<value>#3A4D85</value>
					<value>#82A2FF</value>
					<value>#B8BDCB</value>
					<value>#5D6782</value>
					<value>#838383</value>
					<value>#5A5757</value>
				</default>
			</array>
		</struct>
	</usertypes>
	<modules>
		<module name="profiler" type="profiler"/>
	</modules>
	<validators>
		<handler>sctk_runtime_config_empty_validator_for_test</handler>
	</validators>
</config>
<config name="MPC_Threads">
	<usertypes>
		<struct name="thread" doc="Options for MPC threads.">
			<param name="spin_delay" type="int" default="10" doc="Max number of accesses to the lock before calling thread_yield"/>
			<param name="interval" type="int" default="10" doc=""/>
			<param name="kthread_stack_size" type="size" default="10MB" doc="Define the stack size of MPC user threads"/>
			<param name="placement_policy" type="funcptr" doc="Initialize thread placement policy" default="sctk_get_init_vp_and_nbvp_default"/>
		</struct>
        <struct name="scheduler" doc="Scheduler priority parameters">
            <param name="timestamp_threshold" type="double" default="0.0" doc="Threshold for priority scheduling quantum"/>

            <param name="task_polling_thread_basic_priority" type="int" default="20" doc="Basic priority of polling tasks"/>
            <param name="task_polling_thread_basic_priority_step" type="int" default="20" doc="Step of basic priority of polling tasks"/>
            <param name="task_polling_thread_current_priority_step" type="int" default="20" doc="Step of current priority of polling tasks"/>

            <param name="sched_NBC_Pthread_basic_priority" type="int" default="20" doc="Basic priority of polling tasks"/>
            <param name="sched_NBC_Pthread_basic_priority_step" type="int" default="20" doc="Step of basic priority of nbc progress threads"/>
            <param name="sched_NBC_Pthread_current_priority_step" type="int" default="20" doc="Step of current priority of nbc progress threads"/>

            <param name="mpi_basic_priority" type="int" default="20" doc="Basic priority of MPI threads"/>
            <param name="omp_basic_priority" type="int" default="20" doc="Basic priority of OMP threads"/>
            <param name="posix_basic_priority" type="int" default="20" doc="Basic priority of POSIX threads"/>
            <param name="progress_basic_priority" type="int" default="20" doc="Basic priority of POSIX threads"/>
        </struct>
	</usertypes>
	<modules>
		<module name="thread" type="thread"/>
		<module name="scheduler" type="scheduler"/>
	</modules>
</config>
</all>
