<cvars>
	<collectives>
		<shm_coll>
			<cvar>
				<name>MPI_COLL_TOPO_TREE_ARITY</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Arity being used for topological communicators (can be modifed at runtime between two comms)</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_NO_COMMUTE</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Force the use of deterministic algorithms</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_TOPO_TREE_DUMP</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Activate the DOT dump of the topological tree for debug (topoN.cdat)</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_REDUCE_PIPELINED_BLOCKS</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Number of blocks for pipelined Reduce</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_REDUCE_INTERLEAVE</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Number of reduce slots to allocate</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_REDUCE_INTERLEAVE_TRSH</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_COUNT</datatype>
				<desc>Size required to rely on pipelined reduce (in bytes)</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_BCAST_INTERLEAVE</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Number of bcast slots to allocate</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
		</shm_coll>
		<inter_coll>
			<cvar>
				<name>MPI_COLL_BARRIER_FOR_TRSH</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Comm size threshold to use a for loop for Barrier</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_REDUCE_FOR_ELEM_TRSH</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Element threshold to use a for loop for Reduce</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_REDUCE_FOR_TRSH</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Comm size threshold to use a for loop for Reduce</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
			<cvar>
				<name>MPI_COLL_BCAST_FOR_TRSH</name>
				<verbosity>MPI_T_VERBOSITY_MPIDEV_BASIC</verbosity>
				<datatype>MPI_INT</datatype>
				<desc>Comm size threshold to use a for loop for Bcast</desc>
				<bind>MPI_T_BIND_NO_OBJECT</bind>
				<scope>MPI_T_SCOPE_ALL_EQ</scope>
			</cvar>
		</inter_coll>
	</collectives>
</cvars>
